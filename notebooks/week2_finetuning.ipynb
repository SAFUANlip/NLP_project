{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFGSuogL9Ujo5J2pJWalwf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pl36sKo7k5fo"
      },
      "outputs": [],
      "source": [
        "!pip uninstall unsloth -y\n",
        "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch\n",
        "!pip install --upgrade xformers"
      ],
      "metadata": {
        "id": "zVZeqDMmlE6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchaudio torchvision fastai"
      ],
      "metadata": {
        "id": "qaW3jX7X4Iad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets --upgrade\n",
        "\n",
        "# Install Flash Attention 2 for softcapping support\n",
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ],
      "metadata": {
        "id": "iTD0L6NWlIYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 512\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model_id = \"google/gemma-2-2b\"\n",
        "new_model_id = \"gemma-2-2b-it\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_id,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        "    max_seq_length = max_seq_length,\n",
        ")"
      ],
      "metadata": {
        "id": "vohmhwsFlLmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('neural-bridge/rag-dataset-12000')\n",
        "print(f\"Train dataset size: {len(dataset)} \")"
      ],
      "metadata": {
        "id": "6ZLiDc7dnAK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"Given the following passage, answer the related question.\n",
        "### Passage\n",
        "{}\n",
        "### Question\n",
        "{}\n",
        "### Answer\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "formatting_prompts_func: 各データをプロンプトに合わせた形式に合わせる\n",
        "\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    passage = examples[\"context\"]\n",
        "    question = examples[\"question\"]\n",
        "    answer = examples[\"answer\"]\n",
        "    text = prompt.format(passage, question, answer) + EOS_TOKEN\n",
        "    return { \"formatted_text\" : text, }\n",
        "pass\n",
        "\n",
        "\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    num_proc= 4,\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "eP-GzUc7nt7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][\"formatted_text\"][0])"
      ],
      "metadata": {
        "id": "NnMgOPEYoXr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_text_field=\"formatted_text\",\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        num_train_epochs = 1,\n",
        "        logging_steps = 10,\n",
        "        warmup_steps = 10,\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "        max_steps=-1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        group_by_length=True,\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "_iVgfQ23sHL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "DqyLtZkwsycd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "eGm9a9sis3XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_merged(\n",
        "    new_model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    save_method=\"lora\",\n",
        "    token=\"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\",\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "YX9kFJfAs-Af"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}