{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdHwSH37gAMdTJjThNx2vH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook combines three notebooks: fine-tuning a pretrained model, inference with the fine-tuned model, and inference with the original model.\n",
        "Please run each section separately.\n",
        "\n",
        "To run each section please add your huggingface API key to the secret key HF_TOKEN"
      ],
      "metadata": {
        "id": "DT9mJ6t2RBYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the connection with the Google Dive storage"
      ],
      "metadata": {
        "id": "smb9zqIEKO3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSeRg3F6KQTz",
        "outputId": "271b88b8-d070-4921-a768-15f2c9a3d4b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = 'nlp/Project'\n",
        "\n",
        "os.chdir(f'/content/drive/MyDrive/{path}')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P8RhuCNIKRi7",
        "outputId": "6e66afbf-31cd-4403-ed91-c65b095c8eb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/nlp/Project'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tuning pretrained models\n",
        "In this part, I fine-tuned two pretrained models (gemma-2-2b and Llama-3.2-3B-Instruct) using Retrieval-Augmented Generation (RAG) Dataset 12000\n",
        "\n",
        "Dataset: https://huggingface.co/datasets/neural-bridge/rag-dataset-12000\n",
        "\n",
        "Fine-tuned model:\n",
        "\n",
        "gemma-2-2b: https://huggingface.co/Shodai1122/gemma-2-2b-it\n",
        "\n",
        "Llama-3.2-3B-Instruct: https://huggingface.co/Shodai1122/Llama-3.2-3B-Instruct-it\n",
        "\n",
        "\n",
        "##Unsloth\n",
        "\n",
        "https://github.com/unslothai/unsloth\n",
        "\n",
        "In this notebook I used Unsloth for fine-tuning.\n",
        "Unsloth is a library that significantly accelerates the fine-tuning of large language models (LLMs). Compared to traditional methods, it achieves approximately twice the speed and also reduces memory usage. It combines 4-bit quantization with LoRA technology to achieve both model compression and acceleration."
      ],
      "metadata": {
        "id": "k9fKCExZDHNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_6DhtmZC5it"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch\n",
        "!pip install --upgrade xformers"
      ],
      "metadata": {
        "id": "rWHQMHtYGSYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchaudio torchvision fastai"
      ],
      "metadata": {
        "id": "VEkb_HjHGUUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets --upgrade\n",
        "\n",
        "# Install Flash Attention 2 for softcapping support\n",
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ],
      "metadata": {
        "id": "batEJWGnGWhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 512\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "#model_id = \"google/gemma-2-2b\"\n",
        "new_model_id = \"Llama-3.2-3B-Instruct-it\"\n",
        "#new_model_id = \"gemma-2-2b-it\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_id,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        "    max_seq_length = max_seq_length,\n",
        ")"
      ],
      "metadata": {
        "id": "6GQX93WIGZ_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('neural-bridge/rag-dataset-12000')\n",
        "print(f\"Train dataset size: {len(dataset)} \")"
      ],
      "metadata": {
        "id": "TKKqSNJhGcG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][\"formatted_text\"][0])"
      ],
      "metadata": {
        "id": "J2x0KDKyGhJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_text_field=\"formatted_text\",\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        num_train_epochs = 1,\n",
        "        logging_steps = 10,\n",
        "        warmup_steps = 10,\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "        max_steps=-1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        group_by_length=True,\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "rm63sBP1Gjzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "ujmDz_vCGnJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "1_ybM4fgGqcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_merged(\n",
        "    new_model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    save_method=\"lora\",\n",
        "    token=\"\",#write your huggingface token here\n",
        "    private=True\n",
        ")"
      ],
      "metadata": {
        "id": "tJ5Y3Nb8GrJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference with fine-tuned model"
      ],
      "metadata": {
        "id": "QCGat7k3IvBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "cuTzAbK1I4d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import json"
      ],
      "metadata": {
        "id": "3c_pdVeOJHJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Shodai1122/Llama-3.2-3B-Instruct-it\"\n",
        "#model_name = \"Shodai1122/gemma-2-2b-it\""
      ],
      "metadata": {
        "id": "YP4yYSCsJKj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "e_2OvXT3JOF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "datasets = load_dataset('neural-bridge/rag-dataset-12000', split='test')\n",
        "print(f\"Train dataset size: {len(datasets)} \")"
      ],
      "metadata": {
        "id": "iVN-0Q4BJRDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = datasets.select(range(100))\n",
        "print(len(datasets))"
      ],
      "metadata": {
        "id": "sF_fAIczJTgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasets[0])"
      ],
      "metadata": {
        "id": "EpQOf9ZHJWdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Inference\n",
        "results = []\n",
        "for dt in tqdm(datasets):\n",
        "  context=dt[\"context\"]\n",
        "  question = dt[\"question\"]\n",
        "  answer = dt[\"answer\"]\n",
        "\n",
        "  prompt = f\"\"\"Given the following passage, answer the related question.\\n### Passage\\n{context}\\n### Question\\n{question}\\n### Answer\\n\"\"\"\n",
        "\n",
        "  inputs = tokenizer([prompt], return_tensors = \"pt\").to(model.device)\n",
        "\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True, do_sample=False, repetition_penalty=1.2)\n",
        "  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).split('\\n### Answer')[-1]\n",
        "\n",
        "  results.append({\"question\": question, \"output\": prediction, \"answer\": answer})"
      ],
      "metadata": {
        "id": "GzgnRVcbJXFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"Llama-3.2-3B-Instruct-it_output.jsonl\", 'w', encoding='utf-8') as f:\n",
        "    for result in results:\n",
        "        json.dump(result, f, ensure_ascii=False)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "gn_GTLpqJY0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference with original model"
      ],
      "metadata": {
        "id": "jEZsuF0nJfn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "i3HYDElLJkD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "J2ssBKBdJov-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_id = \"google/gemma-2-2b\"\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ],
      "metadata": {
        "id": "vuZEfHz5JqKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "rJqiu33-Jr3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "datasets = load_dataset('neural-bridge/rag-dataset-12000', split='test')\n",
        "print(f\"Train dataset size: {len(datasets)} \")\n",
        "datasets = datasets.select(range(100))\n",
        "print(len(datasets))"
      ],
      "metadata": {
        "id": "QqrtQOR6JvyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Inference\n",
        "results = []\n",
        "for dt in tqdm(datasets):\n",
        "  context=dt[\"context\"]\n",
        "  question = dt[\"question\"]\n",
        "  answer = dt[\"answer\"]\n",
        "\n",
        "  prompt = f\"\"\"Given the following passage, answer the related question.\\n### Passage\\n{context}\\n### Question\\n{question}\\n### Answer\\n\"\"\"\n",
        "\n",
        "  inputs = tokenizer([prompt], return_tensors = \"pt\").to(model.device)\n",
        "\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True, do_sample=False, repetition_penalty=1.2)\n",
        "  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).split('\\n### Answer')[-1]\n",
        "\n",
        "  results.append({\"question\": question, \"output\": prediction, \"answer\": answer})"
      ],
      "metadata": {
        "id": "e9-OUXRnJxZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"Llama-3.2-3B-Instruct_output.jsonl\", 'w', encoding='utf-8') as f:\n",
        "    for result in results:\n",
        "        json.dump(result, f, ensure_ascii=False)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "H9PVBmsyJ0LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare output of original model and fine-tuned model\n",
        "For inference I used first 100 test datasets from Retrieval-Augmented Generation (RAG) Dataset 12000\n",
        "\n",
        "Dataset: https://huggingface.co/datasets/neural-bridge/rag-dataset-12000\n",
        "\n",
        "\n",
        "gemma-2-2b"
      ],
      "metadata": {
        "id": "M6PJNSoBN6vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_path = 'gemma-2-2b_output.jsonl'\n",
        "\n",
        "gemma_output = []\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            gemma_output.append(json.loads(line))"
      ],
      "metadata": {
        "id": "9mZmVe9NMSkM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'gemma-2-2b-it_output.jsonl'\n",
        "\n",
        "gemma_it_output = []\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            gemma_it_output.append(json.loads(line))"
      ],
      "metadata": {
        "id": "zGJ1kZA3Mt1a"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question:\",gemma_output[0]['question'])\n",
        "print(\"\\n\")\n",
        "print(\"Output of original gemma model:\",gemma_output[0]['output'])\n",
        "print(\"\\n\")\n",
        "print(\"Output of fine-tuned gemma model:\",gemma_it_output[0]['output'])\n",
        "print(\"Answer (GPT-4):\",gemma_output[0]['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_QlI2_8NCfD",
        "outputId": "f62b1a00-861f-4770-d205-a73e576402ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who is the music director of the Quebec Symphony Orchestra?\n",
            "\n",
            "\n",
            "Output of original gemma model: \n",
            "a.) Jean-François Rivet b.) Gilles Apap c.) Laurent Piveron d.) Fabien Gabel e.) None of these\n",
            "\n",
            "\n",
            "Output of fine-tuned gemma model: \n",
            "Fabien Gabel is the music director of the Quebec Symphony Orchestra.\n",
            "\n",
            "Answer (GPT-4): The music director of the Quebec Symphony Orchestra is Fabien Gabel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama-3.2-3B"
      ],
      "metadata": {
        "id": "YhDVnbxsONoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'Llama-3.2-3B-Instruct_output.jsonl'\n",
        "\n",
        "llama_output = []\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            llama_output.append(json.loads(line))"
      ],
      "metadata": {
        "id": "q8nw1HeEOSpo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'Llama-3.2-3B-Instruct-it_output.jsonl'\n",
        "\n",
        "llama_it_output = []\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            llama_it_output.append(json.loads(line))"
      ],
      "metadata": {
        "id": "v7LeHpU9OptT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question:\",llama_output[0]['question'])\n",
        "print(\"\\n\")\n",
        "print(\"Output of original llama model:\",llama_output[0]['output'])\n",
        "print(\"\\n\")\n",
        "print(\"Output of fine-tuned llama model:\",llama_it_output[0]['output'])\n",
        "print(\"Answer (GPT-4):\",llama_output[0]['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LI819rxO_gR",
        "outputId": "1071bf28-d259-46bd-e942-9f18acfeb5c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who is the music director of the Quebec Symphony Orchestra?\n",
            "\n",
            "\n",
            "Output of original llama model: \n",
            "According to the passage, Fabien Gabel is the music director of the Quebec Symphony Orchestra.\n",
            "\n",
            "\n",
            "Output of fine-tuned llama model: \n",
            "Fabien Gabel is the music director of the Quebec Symphony Orchestra.\n",
            "\n",
            "Answer (GPT-4): The music director of the Quebec Symphony Orchestra is Fabien Gabel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare all models"
      ],
      "metadata": {
        "id": "K8YDCO2dPobh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_num = 2 #Choose question number from 0 to 99\n",
        "\n",
        "print(\"Question:\",gemma_output[question_num]['question'])\n",
        "print(\"\\n\")\n",
        "print(\"Output of original gemma model:\",gemma_output[question_num]['output'])\n",
        "print(\"\\n\")\n",
        "print(\"Output of fine-tuned gemma model:\",gemma_it_output[question_num]['output'])\n",
        "print(\"\\n\")\n",
        "print(\"Output of original llama model:\",llama_output[question_num]['output'])\n",
        "print(\"\\n\")\n",
        "print(\"Output of fine-tuned llama model:\",llama_it_output[question_num]['output'])\n",
        "print(\"Answer (GPT-4):\",gemma_output[question_num]['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6XJ_xXAP-My",
        "outputId": "ed55c9b1-8a11-49d8-b94a-4e0c4d767047"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What did Paul Wall offer to all U.S. Olympic Medalists?\n",
            "\n",
            "\n",
            "Output of original gemma model: \n",
            "A gold grill\n",
            "\n",
            "\n",
            "Output of fine-tuned gemma model: \n",
            "He offered free gold grills to all U.S. Olympic Medalists.\n",
            "\n",
            "\n",
            "\n",
            "Output of original llama model: \n",
            "According to the passage, Paul Wall offered free gold grills to any team USA member who wins gold.\n",
            "\n",
            "\n",
            "Output of fine-tuned llama model: \n",
            "Paul Wall promised his team he would give free gold grills to any U.S. Olympic medalist if they won the AAC title.\n",
            "\n",
            "Answer (GPT-4): Paul Wall wants to give free gold grills to all U.S. Olympic Medalists.\n"
          ]
        }
      ]
    }
  ]
}