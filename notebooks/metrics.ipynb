{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e169cbe6-9d42-4eba-adca-aefa24ee4499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safuan/Python/AI/venv_ai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "import warnings\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c44abf5-6e76-433a-8467-7971c829bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab702da-f376-4b44-bd3f-c0fccb99a9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 2400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea754edd-6ade-4ab0-9b37-342f25d5a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_list_format(dataset_dict):\n",
    "    train_ds_, test_ds_ = [], []\n",
    "    \n",
    "    # Process train split\n",
    "    for sample in dataset_dict['train']:\n",
    "        if sample['answer'] is not None:\n",
    "            train_ds_.append({\n",
    "                \"user_input\": sample['question'],\n",
    "                \"response\": sample['answer'],\n",
    "                \"reference\": sample['answer'][:len(sample['answer'])-5],  # Using answer as reference for now\n",
    "                \"retrieved_contexts\": [sample['context']],\n",
    "                \"reference_contexts\": [sample['context']]\n",
    "            })\n",
    "    \n",
    "    # Process test split\n",
    "    for sample in dataset_dict['test']:\n",
    "        if sample['answer'] is not None:\n",
    "            test_ds_.append({\n",
    "                \"user_input\": sample['question'],\n",
    "                \"response\": sample['answer'],\n",
    "                \"reference\": sample['answer'][:len(sample['answer'])-5],  # Using answer as reference for now\n",
    "                \"retrieved_contexts\": [sample['context']],\n",
    "                \"reference_contexts\": [sample['context']]\n",
    "            })\n",
    "    \n",
    "    return train_ds_, test_ds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac6b856-209b-46e0-a661-4e6049507061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the dataset\n",
    "train_ds, test_ds = convert_dataset_to_list_format(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc6bc05-03eb-419a-9d7c-aa7432e359b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2399"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73220a6d-2b6e-4c54-a751-7649639ae99f",
   "metadata": {},
   "source": [
    "# [RAG (Retrieval-Augmented Generation) metrics (LLM based)](https://medium.com/@med.el.harchaoui/rag-evaluation-metrics-explained-a-complete-guide-dbd7a3b571a8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00a2dc-a8a5-4d87-b642-4b6ff895bd8e",
   "metadata": {},
   "source": [
    "![RAG_scheme](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hA2RpiXjL3dvm--v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283dd12-c3a2-45d4-80e8-942d0e99f174",
   "metadata": {},
   "source": [
    "### For now we will more concentrate on evaluation of Generation part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9381ee8-0b1d-40cc-8de2-ff94f4cfecc0",
   "metadata": {},
   "source": [
    "## Faithfulness\n",
    "\n",
    "This metric measure how the LLM answer is faithful to the provided context, does it respect what was given as input or not. Its considered as faithful if the claims made in the answer can be extracted from the provided context. To calculate it, we start by extracting all claims from the LLM provided answer first. Then for each claim we check if this one claim can be inferred from the retrieved context. It value range from 0 to 1. Higher is better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fe285cf-c2f0-48a7-b657-527e6fdf74b5",
   "metadata": {},
   "source": [
    "![Faithulness](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g8cCq5m5Fz2XOPBy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b1e99-7874-4db3-ab44-ebbbbedfb019",
   "metadata": {},
   "source": [
    "## Answer Relevance\n",
    "\n",
    "This metric measure the quality of the generated answer given the user query, how pertinent is the answer with respect the the user question. To assess this we need to know if the answer is complete or not, does it contain redundant information ?\n",
    "\n",
    "To calculate this metric, we generate N question based on the answer, does questions should be normally similar the the original question if the provided answer is relevant to the original question, if not they will be different. To compare the N generated question, we use cosine or dot product vector similarity operators. The value should range between 0 and 1.\n",
    "The formula for determining answer relevance is as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68cf7531-d7ea-4997-ad1c-8fae7129470c",
   "metadata": {},
   "source": [
    "![Answer Relevance](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QffodGkNYSRzcH52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceffee02-0e78-410a-bba6-eee559b4bdc4",
   "metadata": {},
   "source": [
    "# Traditional NLP metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc3580-e3ed-43ff-8dfb-fa63c376bc83",
   "metadata": {},
   "source": [
    "## String simillarity \n",
    "\n",
    "Metric measures the similarity between the reference and the response using traditional string distance measures such as Levenshtein, Hamming, and [Jaro](https://srinivas-kulkarni.medium.com/jaro-winkler-vs-levenshtein-distance-2eab21832fd6)\n",
    "\n",
    "## [BLEU](https://medium.com/nlplanet/two-minutes-nlp-learn-the-bleu-metric-by-examples-df015ca73a86)(Bilingual Evaluation Understudy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3179e41-ac8c-4544-a6e2-5a4d2deaab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample, EvaluationDataset, evaluate\n",
    "\n",
    "#Traditional metrics\n",
    "from ragas.metrics._string import NonLLMStringSimilarity, DistanceMeasure\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    \n",
    ")\n",
    "from ragas.metrics import BleuScore, RougeScore\n",
    "\n",
    "# LLM metrics\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import Faithfulness, FaithfulnesswithHHEM, ResponseRelevancy\n",
    "\n",
    "# Choose the appropriate import based on your API:\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from ragas import evaluate\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a5d017-3f97-49ca-84dd-6dd3fbae2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to uload Dashboard on ragas website\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = \"apt.4955-d3328fbcd0ba-45a7-8140-094d1ba2-fb3b7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b181ab-7dc7-410c-8e82-123d6176c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = EvaluationDataset.from_list(test_ds[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d89851a-2337-48cb-aed2-d69a61f50fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tr/d1rx7zt15lzgkp2yc1nwbg100000gn/T/ipykernel_5348/3259155587.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  langchain_llm = ChatOllama(model=\"llama3.2:1b\")\n",
      "/var/folders/tr/d1rx7zt15lzgkp2yc1nwbg100000gn/T/ipykernel_5348/3259155587.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  langchain_embeddings = OllamaEmbeddings(model=\"llama3.2:1b\")\n"
     ]
    }
   ],
   "source": [
    "langchain_llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "langchain_embeddings = OllamaEmbeddings(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea97a1d-f783-4532-98ef-7a484aac7b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|████████████████▌                | 2/4 [00:23<00:22, 11.48s/it]Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[3]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  75%|████████████████████████▊        | 3/4 [00:29<00:09,  9.18s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Evaluating: 100%|█████████████████████████████████| 4/4 [03:00<00:00, 45.00s/it]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "  eval_ds,\n",
    "  metrics=[\n",
    "    # BleuScore(),\n",
    "    # RougeScore(),\n",
    "    # NonLLMStringSimilarity(distance_measure=DistanceMeasure.LEVENSHTEIN),\n",
    "    #ResponseRelevancy(),\n",
    "    # context_precision,\n",
    "    faithfulness,\n",
    "    # answer_relevancy,\n",
    "    # context_recall\n",
    "  ], \n",
    "    llm=langchain_llm,\n",
    "    embeddings=langchain_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f028de48-ef26-4435-b2c6-f8752b36e1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.2500}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5927999a-d32e-41cb-97ab-b7cd7b6222d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with Google AI Studio\n",
    "Gemini_OPEN_API_KEY = \"AIzaSyAej7Q9cR5zcFginFZ16o2LBf14mxhG4Ok\"\n",
    "\n",
    "config = {\n",
    "    \"model\": \"gemini-1.5-flash\",  # or other model IDs\n",
    "    \"temperature\": 0.4,\n",
    "    \"max_tokens\": None,\n",
    "    \"top_p\": 0.8,\n",
    "}\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatGoogleGenerativeAI(\n",
    "    model=config[\"model\"],\n",
    "    api_key = Gemini_OPEN_API_KEY,\n",
    "    temperature=config[\"temperature\"],\n",
    "    max_tokens=config[\"max_tokens\"],\n",
    "    top_p=config[\"top_p\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440ed3c6-0e40-47c2-8626-ebee53c5a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"credentials_profile_name\": \"your-profile-name\",  # E.g \"default\"\n",
    "    \"region_name\": \"your-region-name\",  # E.g. \"us-east-1\"\n",
    "    \"llm\": \"your-llm-model-id\",  # E.g \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "    \"embeddings\": \"your-embedding-model-id\",  # E.g \"amazon.titan-embed-text-v2:0\"\n",
    "    \"temperature\": 0.4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d632e5-a88d-45e1-8a38-887a0d5a6bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bef5d1f-90a8-431d-994c-ea49ba7f3d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2399"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae716c-c4a0-4628-801f-fb57662d24e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                            | 1/9596 [00:01<4:08:33,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "results = evaluate(dataset=eval_ds, metrics=[\n",
    "    BleuScore(),\n",
    "    RougeScore(),\n",
    "    NonLLMStringSimilarity(distance_measure=DistanceMeasure.LEVENSHTEIN),\n",
    "    #FaithfulnesswithHHEM(llm=evaluator_llm)\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94edd9-3beb-48d5-929c-a094eeb62753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2940137e-9f6a-4327-998b-eca0a207b9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = Faithfulness(llm=evaluator_llm)\n",
    "await scorer.single_turn_ascore(eval_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cff6fa7-1759-4333-b02b-17b759f30496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google AI Studio Embeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(GoogleGenerativeAIEmbeddings(\n",
    "    google_api_key = Gemini_OPEN_API_KEY,\n",
    "    model=\"models/embedding-001\",  # Google's text embedding model\n",
    "    task_type=\"retrieval_document\"  # Optional: specify the task type\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e15e8d-510c-46e5-b3ce-f45b1cdc600a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "test_data = {\n",
    "    \"user_input\": \"summarise given text\\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.\",\n",
    "    \"response\": \"The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.\",\n",
    "}\n",
    "\n",
    "metric = AspectCritic(name=\"summary_accuracy\", llm=evaluator_llm, definition=\"Verify if the summary is accurate.\")\n",
    "test_data = SingleTurnSample(**test_data)\n",
    "await metric.single_turn_ascore(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1028023d-f9ce-4c34-a03b-43a2f50be3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████████████████████| 2/2 [28:47<00:00, 863.79s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "def evaluate_rag_result(query, context, answer):\n",
    "    inputs = tokenizer(f\"{query}\\nContext: {context}\\nAnswer: {answer}\", return_tensors=\"pt\")\n",
    "    scores = model(**inputs).logits\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eab5d83-62fe-4d81-9f9e-31a3add8391a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0366, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rag_result(test_ds[0][\"user_input\"], \n",
    "                    test_ds[0][\"retrieved_contexts\"],\n",
    "                    test_ds[0][\"response\"],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90ea3d9f-bb0c-4928-9360-db96b54533b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adfda325-e52a-4384-8ef9-479784bbee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "# Create the generation pipeline\n",
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.4,\n",
    "    top_p=0.8,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Wrap in LangChain's LLM interface\n",
    "langchain_llm = HuggingFacePipeline(pipeline=generation_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ef9202d-dd4c-4f87-a4f3-e8b8d3b85491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(langchain_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37281182-bde1-4bb4-a5af-91b524ecc803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my env ai",
   "language": "python",
   "name": "myenv_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
