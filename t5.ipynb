{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ed0bbc",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to fine-tune a pre-trained **Flan-T5** model for a summarization task using **LoRA** — a Parameter-Efficient Fine-Tuning (PEFT) technique. Instead of updating all model parameters, we fine-tune only small trainable adapters injected into selected layers of the transformer, reducing the computational and memory cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca42033",
   "metadata": {},
   "source": [
    "Theoretical Background\n",
    "\n",
    "What is PEFT?\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning (PEFT)** refers to methods that allow training large language models (LLMs) by modifying only a **small subset of parameters**.\n",
    "\n",
    "Traditional fine-tuning requires updating **all parameters** of the model, which is inefficient and costly for LLMs. PEFT techniques overcome this by introducing a small number of **trainable parameters** (e.g., adapters, prompts, LoRA layers), leaving the backbone model **frozen**.\n",
    "\n",
    "What is LoRA?\n",
    "\n",
    "**Low-Rank Adaptation (LoRA)** is a PEFT method that modifies **attention layers** of transformer models.\n",
    "\n",
    "Instead of fine-tuning the full weight matrix \\( W \\), LoRA freezes it and injects a low-rank decomposition:\n",
    "\n",
    "$$\n",
    "W' = W + \\Delta W = W + AB\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "A \\in \\mathbb{R}^{d \\times r}\n",
    "$$\n",
    "\n",
    "$$\n",
    "B \\in \\mathbb{R}^{r \\times k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "r \\ll d, k\n",
    "$$\n",
    "\n",
    "- The **rank** \\( r \\) is typically between 4 and 16.\n",
    "- Only matrices **A** and **B** are trainable, making this approach very lightweight.\n",
    "\n",
    "**LoRA advantages**:\n",
    "- Reduces trainable parameters by 10x–1000x\n",
    "- Compatible with most transformer models\n",
    "- Easy to merge back into base model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3250ec",
   "metadata": {},
   "source": [
    "---\n",
    "Project Pipeline\n",
    "\n",
    "1. **Model & Tokenizer**: Load `google/flan-t5-base` with `AutoAdapterModel`.\n",
    "2. **Dataset**: Use `\"neural-bridge/rag-dataset-12000\"` with context, question, and answer fields.\n",
    "3. **Preprocessing**:\n",
    "   - Concatenate question + context\n",
    "   - Add task prefix `\"summarize: \"`\n",
    "   - Tokenize inputs/targets\n",
    "4. **Adapter Injection**:\n",
    "   - Define `LoRAConfig` (e.g., `r=8`, `alpha=16`)\n",
    "   - Add and activate LoRA adapter\n",
    "5. **Training**:\n",
    "   - Use `AdapterTrainer` with 2 epochs and small batch size\n",
    "6. **Inference**:\n",
    "   - Test on custom story + question\n",
    "   - Use `generate()` for summarization\n",
    "7. **Adapter Merge**:\n",
    "   - Merge trained LoRA weights into base model for deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30269db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "prefix = 'summarize: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(examples):\n",
    "    text_column1 = 'context'\n",
    "    text_column2 = 'question'\n",
    "    summary_column = 'answer'\n",
    "    \n",
    "    padding = \"max_length\"\n",
    "\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column1])):\n",
    "        if examples[text_column1][i] and examples[text_column2][i] and examples[summary_column][i]:\n",
    "            # Concatenate question + context\n",
    "            input_text = examples[text_column2][i] + \" \" + examples[text_column1][i]\n",
    "            inputs.append(input_text)\n",
    "            targets.append(examples[summary_column][i])\n",
    "\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding=padding, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, padding=padding, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffec54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split_name, max_items):\n",
    "    \n",
    "    dataset = load_dataset(\"neural-bridge/rag-dataset-12000\")[split_name] \n",
    "\n",
    "\n",
    "    dataset = dataset.filter(lambda example: example['context'] is not None and example['answer'] is not None)\n",
    "    \n",
    "    dataset = dataset.filter(lambda _, idx: idx < max_items, with_indices=True)\n",
    "    \n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        encode_batch,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Running tokenizer on \" + split_name + \" dataset\",\n",
    "    )\n",
    "    \n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ee3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForSeq2SeqLM\n",
    "from adapters import LoRAConfig\n",
    "\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(base_model)\n",
    "\n",
    "# Load the model\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
    "\n",
    "config = LoRAConfig(\n",
    "    r=8,\n",
    "    alpha=16,\n",
    "    intermediate_lora=True,\n",
    "    output_lora=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "044b9936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b49ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'adapters.models.t5.adapter_model.T5AdapterModel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3de852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add_adapter(\"my_summary_adapter\", config=config, adapter_type=\"lora\")\n",
    "model.add_adapter(adapter_name=\"my_summary_adapter\", config=config)\n",
    "\n",
    "model.train_adapter(\"my_summary_adapter\")\n",
    "model.set_active_adapters(\"my_summary_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57144e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e47ab3f5a04eb98ae60343839c9b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3549dff46e4ce6b781abd9f8e3efe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc7cebef7b244678eba63f3a0d75fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e10caf5fba849beaf0a99f5d507b267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623d2d0a50d7458b923d4fe74d3ed61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89faa17c7b2e4183bb56cb43efbcf4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on test dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>23.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.827300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.527400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.422000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=2.032686315536499, metrics={'train_runtime': 383.374, 'train_samples_per_second': 5.217, 'train_steps_per_second': 2.608, 'total_flos': 1381594300416000.0, 'train_loss': 2.032686315536499, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from adapters import AdapterTrainer\n",
    "from datasets import load_dataset\n",
    "batch_size = 2  \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=50,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=load_split(\"train\", 1000),\n",
    "    eval_dataset=load_split(\"test\", 100),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4e2302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3849603533744812,\n",
       " 'eval_runtime': 6.2349,\n",
       " 'eval_samples_per_second': 16.039,\n",
       " 'eval_steps_per_second': 8.019,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec80e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.merge_adapter(\"my_summary_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ea6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " summarize: Summarize the story. \n",
      "Once upon a time, there were two brothers — one was rich, and the other was poor. The poor brother ran out of food and went to his rich brother, begging for something to eat.\n",
      "\n",
      "The rich brother, not happy about helping, said, “I’ll give you this ham, but you must take it to Dead Man’s Hall.”\n",
      "\n",
      "Grateful for the food, the poor brother agreed. He walked all day and finally reached a large building at dusk. Outside, an old man was chopping wood.\n",
      "\n",
      "“Excuse me, sir,” said the poor brother. “Is this the way to Dead Man’s Hall?”\n",
      "\n",
      "“Yes, you’ve arrived,” replied the old man. “Inside, they will want to buy your ham. But don’t sell it unless they give you the hand-mill that stands behind the door.”\n",
      "\n",
      "The poor brother thanked the old man, went inside, and everything happened just as the old man had said. The poor brother left with the hand-mill and asked the old man how to use it. Then, he set off home.\n",
      "\n",
      "The hand-mill was magical. When the poor brother got home, he asked it to grind a feast of food and drink. To stop the mill, he simply had to say, “Thank you, magic mill, you can stop now.”\n",
      "\n",
      "When the rich brother saw that his brother was no longer poor, he became jealous. “Give me that mill!” he demanded. The poor brother, having everything he needed, agreed to sell it but didn’t tell his rich brother how to stop it.\n",
      "\n",
      "The rich brother eagerly asked the mill to grind food when he got home, but because he didn’t know how to stop it, the mill kept grinding until food overflowed from the house and across the fields. In a panic, he ran to his poor brother’s house. “Please take it back!” he cried. “If it doesn’t stop, the whole town will be buried!”\n",
      "\n",
      "The poor brother took the mill back and was never poor or hungry again.\n",
      "\n",
      "Soon, the story of the magic mill spread far and wide. One day, a sailor knocked at the poor brother’s door. “Does the mill grind salt?” he asked.\n",
      "\n",
      "“Of course,” replied the brother. “It will grind anything you ask.”\n",
      "\n",
      "The sailor, eager to stop traveling far for salt, offered a thousand coins for the mill. Though the brother was hesitant, he eventually agreed.\n",
      "\n",
      "In his hurry, the sailor forgot to ask how to stop the mill. Once at sea, he placed the mill on deck and commanded, “Grind salt, and grind quickly!”\n",
      "\n",
      "The mill obeyed, but it didn’t stop. The pile of salt grew and grew until the ship sank under its weight.\n",
      "\n",
      "The mill still lies at the bottom of the sea, grinding salt to this day, and that’s why the sea is salty.\n",
      "\n",
      "\n",
      "\n",
      "Generated Summary:\n",
      " The story of the magic mill spread far and wide.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Once upon a time, there were two brothers — one was rich, and the other was poor. The poor brother ran out of food and went to his rich brother, begging for something to eat.\n",
    "\n",
    "The rich brother, not happy about helping, said, “I’ll give you this ham, but you must take it to Dead Man’s Hall.”\n",
    "\n",
    "Grateful for the food, the poor brother agreed. He walked all day and finally reached a large building at dusk. Outside, an old man was chopping wood.\n",
    "\n",
    "“Excuse me, sir,” said the poor brother. “Is this the way to Dead Man’s Hall?”\n",
    "\n",
    "“Yes, you’ve arrived,” replied the old man. “Inside, they will want to buy your ham. But don’t sell it unless they give you the hand-mill that stands behind the door.”\n",
    "\n",
    "The poor brother thanked the old man, went inside, and everything happened just as the old man had said. The poor brother left with the hand-mill and asked the old man how to use it. Then, he set off home.\n",
    "\n",
    "The hand-mill was magical. When the poor brother got home, he asked it to grind a feast of food and drink. To stop the mill, he simply had to say, “Thank you, magic mill, you can stop now.”\n",
    "\n",
    "When the rich brother saw that his brother was no longer poor, he became jealous. “Give me that mill!” he demanded. The poor brother, having everything he needed, agreed to sell it but didn’t tell his rich brother how to stop it.\n",
    "\n",
    "The rich brother eagerly asked the mill to grind food when he got home, but because he didn’t know how to stop it, the mill kept grinding until food overflowed from the house and across the fields. In a panic, he ran to his poor brother’s house. “Please take it back!” he cried. “If it doesn’t stop, the whole town will be buried!”\n",
    "\n",
    "The poor brother took the mill back and was never poor or hungry again.\n",
    "\n",
    "Soon, the story of the magic mill spread far and wide. One day, a sailor knocked at the poor brother’s door. “Does the mill grind salt?” he asked.\n",
    "\n",
    "“Of course,” replied the brother. “It will grind anything you ask.”\n",
    "\n",
    "The sailor, eager to stop traveling far for salt, offered a thousand coins for the mill. Though the brother was hesitant, he eventually agreed.\n",
    "\n",
    "In his hurry, the sailor forgot to ask how to stop the mill. Once at sea, he placed the mill on deck and commanded, “Grind salt, and grind quickly!”\n",
    "\n",
    "The mill obeyed, but it didn’t stop. The pile of salt grew and grew until the ship sank under its weight.\n",
    "\n",
    "The mill still lies at the bottom of the sea, grinding salt to this day, and that’s why the sea is salty.\n",
    "\n",
    "\"\"\"\n",
    "question = \"Summarize the story.\"\n",
    "\n",
    "input_text = prefix + question + \" \" + context\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_length=128)\n",
    "\n",
    "generated_summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Input:\\n\", input_text)\n",
    "print(\"\\nGenerated Summary:\\n\", generated_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
